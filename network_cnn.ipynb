{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOO6eIzYcpUqrPTQ1+6S0z8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ReasonDuan/machine-learning-code/blob/main/network_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 公共方法"
      ],
      "metadata": {
        "id": "qiB_GxGEEh0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def identity_function(x):\n",
        "  return x\n",
        "\n",
        "\n",
        "def step_function(x):\n",
        "  return np.array(x > 0, dtype=np.int)\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "def sigmoid_grad(x):\n",
        "  return (1.0 - sigmoid(x)) * sigmoid(x)\n",
        "\n",
        "\n",
        "def relu(x):\n",
        "  return np.maximum(0, x)\n",
        "\n",
        "\n",
        "def relu_grad(x):\n",
        "  grad = np.zeros(x)\n",
        "  grad[x>=0] = 1\n",
        "  return grad\n",
        "\n",
        "\n",
        "def softmax(x):\n",
        "  if x.ndim == 2:\n",
        "    x = x.T\n",
        "    x = x - np.max(x, axis=0)\n",
        "    y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "    return y.T\n",
        "\n",
        "  x = x - np.max(x) # 溢出对策\n",
        "  return np.exp(x) / np.sum(np.exp(x))\n",
        "\n",
        "\n",
        "def mean_squared_error(y, t):\n",
        "  return 0.5 * np.sum((y-t)**2)\n",
        "\n",
        "\n",
        "def cross_entropy_error(y, t):\n",
        "  if y.ndim == 1:\n",
        "    t = t.reshape(1, t.size)\n",
        "    y = y.reshape(1, y.size)\n",
        "\n",
        "  # 监督数据是one-hot-vector的情况下，转换为正确解标签的索引\n",
        "  if t.size == y.size:\n",
        "    t = t.argmax(axis=1)\n",
        "\n",
        "  batch_size = y.shape[0]\n",
        "  return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
        "\n",
        "\n",
        "def softmax_loss(X, t):\n",
        "  y = softmax(X)\n",
        "  return cross_entropy_error(y, t)\n"
      ],
      "metadata": {
        "id": "LRCOYv9SEl7Z"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 工具类"
      ],
      "metadata": {
        "id": "irl41eIED6tC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def smooth_curve(x):\n",
        "  \"\"\"用于使损失函数的图形变圆滑\n",
        "\n",
        "  参考：http://glowingpython.blogspot.jp/2012/02/convolution-with-numpy.html\n",
        "  \"\"\"\n",
        "  window_len = 11\n",
        "  s = np.r_[x[window_len-1:0:-1], x, x[-1:-window_len:-1]]\n",
        "  w = np.kaiser(window_len, 2)\n",
        "  y = np.convolve(w/w.sum(), s, mode='valid')\n",
        "  return y[5:len(y)-5]\n",
        "\n",
        "\n",
        "def shuffle_dataset(x, t):\n",
        "  \"\"\"打乱数据集\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  x : 训练数据\n",
        "  t : 监督数据\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  x, t : 打乱的训练数据和监督数据\n",
        "  \"\"\"\n",
        "  permutation = np.random.permutation(x.shape[0])\n",
        "  x = x[permutation,:] if x.ndim == 2 else x[permutation,:,:,:]\n",
        "  t = t[permutation]\n",
        "\n",
        "  return x, t\n",
        "\n",
        "def conv_output_size(input_size, filter_size, stride=1, pad=0):\n",
        "  return (input_size + 2*pad - filter_size) / stride + 1\n",
        "\n",
        "\n",
        "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
        "  \"\"\"\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  input_data : 由(数据量, 通道, 高, 长)的4维数组构成的输入数据\n",
        "  filter_h : 滤波器的高\n",
        "  filter_w : 滤波器的长\n",
        "  stride : 步幅\n",
        "  pad : 填充\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  col : 2维数组\n",
        "  \"\"\"\n",
        "  N, C, H, W = input_data.shape\n",
        "  out_h = (H + 2*pad - filter_h)//stride + 1\n",
        "  out_w = (W + 2*pad - filter_w)//stride + 1\n",
        "\n",
        "  img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
        "  col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
        "\n",
        "  for y in range(filter_h):\n",
        "    y_max = y + stride*out_h\n",
        "    for x in range(filter_w):\n",
        "      x_max = x + stride*out_w\n",
        "      col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
        "\n",
        "  col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
        "  return col\n",
        "\n",
        "\n",
        "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
        "  \"\"\"\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  col :\n",
        "  input_shape : 输入数据的形状（例：(10, 1, 28, 28)）\n",
        "  filter_h :\n",
        "  filter_w\n",
        "  stride\n",
        "  pad\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "\n",
        "  \"\"\"\n",
        "  N, C, H, W = input_shape\n",
        "  out_h = (H + 2*pad - filter_h)//stride + 1\n",
        "  out_w = (W + 2*pad - filter_w)//stride + 1\n",
        "  col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
        "\n",
        "  img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
        "  for y in range(filter_h):\n",
        "    y_max = y + stride*out_h\n",
        "    for x in range(filter_w):\n",
        "      x_max = x + stride*out_w\n",
        "      img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
        "\n",
        "  return img[:, :, pad:H + pad, pad:W + pad]"
      ],
      "metadata": {
        "id": "rwA2GgQFD7m4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 加载数据"
      ],
      "metadata": {
        "id": "iiiGvryuO9iv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import requests\n",
        "import os.path\n",
        "import gzip\n",
        "import pickle\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# https://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
        "url_base = 'https://ossci-datasets.s3.amazonaws.com/mnist/'  # mirror site\n",
        "key_file = {\n",
        "    'train_img':'train-images-idx3-ubyte.gz',\n",
        "    'train_label':'train-labels-idx1-ubyte.gz',\n",
        "    'test_img':'t10k-images-idx3-ubyte.gz',\n",
        "    'test_label':'t10k-labels-idx1-ubyte.gz'\n",
        "}\n",
        "\n",
        "dataset_dir = './data'\n",
        "save_file = dataset_dir + \"/mnist.pkl\"\n",
        "\n",
        "train_num = 60000\n",
        "test_num = 10000\n",
        "img_dim = (1, 28, 28)\n",
        "img_size = 784\n",
        "\n",
        "\n",
        "def _download(file_name):\n",
        "\n",
        "  # 确保目录存在\n",
        "  os.makedirs(dataset_dir, exist_ok=True)\n",
        "\n",
        "  save_path = os.path.join(dataset_dir, file_name)\n",
        "\n",
        "  # 判断文件是否存在\n",
        "  if os.path.exists(save_path):\n",
        "    return\n",
        "  # 下载文件\n",
        "  response = requests.get(url_base + file_name, stream=True)\n",
        "\n",
        "  # 将内容保存到指定目录下\n",
        "  with open(save_path, 'wb') as file:\n",
        "    for chunk in response.iter_content(chunk_size=1024):\n",
        "      if chunk:\n",
        "        file.write(chunk)\n",
        "\n",
        "  print(f\"文件已下载并保存到: {save_path}\")\n",
        "\n",
        "\n",
        "def download_mnist():\n",
        "  for v in key_file.values():\n",
        "    _download(v)\n",
        "\n",
        "def _load_label(file_name):\n",
        "  file_path = dataset_dir + \"/\" + file_name\n",
        "\n",
        "  print(\"Converting \" + file_name + \" to NumPy Array ...\")\n",
        "  with gzip.open(file_path, 'rb') as f:\n",
        "    labels = np.frombuffer(f.read(), np.uint8, offset=8)\n",
        "  print(\"Done\")\n",
        "\n",
        "  return labels\n",
        "\n",
        "def _load_img(file_name):\n",
        "  file_path = dataset_dir + \"/\" + file_name\n",
        "\n",
        "  print(\"Converting \" + file_name + \" to NumPy Array ...\")\n",
        "  with gzip.open(file_path, 'rb') as f:\n",
        "    data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
        "  data = data.reshape(-1, img_size)\n",
        "  print(\"Done\")\n",
        "\n",
        "  return data\n",
        "\n",
        "def _convert_numpy():\n",
        "  dataset = {}\n",
        "  dataset['train_img'] =  _load_img(key_file['train_img'])\n",
        "  dataset['train_label'] = _load_label(key_file['train_label'])\n",
        "  dataset['test_img'] = _load_img(key_file['test_img'])\n",
        "  dataset['test_label'] = _load_label(key_file['test_label'])\n",
        "\n",
        "  return dataset\n",
        "\n",
        "def init_mnist():\n",
        "  download_mnist()\n",
        "  dataset = _convert_numpy()\n",
        "  print(\"Creating pickle file ...\")\n",
        "  with open(save_file, 'wb') as f:\n",
        "    pickle.dump(dataset, f, -1)\n",
        "  print(\"Done!\")\n",
        "\n",
        "def _change_one_hot_label(X):\n",
        "  T = np.zeros((X.size, 10))\n",
        "  for idx, row in enumerate(T):\n",
        "      row[X[idx]] = 1\n",
        "\n",
        "  return T\n",
        "\n",
        "\n",
        "def load_mnist(normalize=True, flatten=True, one_hot_label=False):\n",
        "  \"\"\"读入MNIST数据集\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  normalize : 将图像的像素值正规化为0.0~1.0\n",
        "  one_hot_label :\n",
        "    one_hot_label为True的情况下，标签作为one-hot数组返回\n",
        "    one-hot数组是指[0,0,1,0,0,0,0,0,0,0]这样的数组\n",
        "  flatten : 是否将图像展开为一维数组\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  (训练图像, 训练标签), (测试图像, 测试标签)\n",
        "  \"\"\"\n",
        "  if not os.path.exists(save_file):\n",
        "    init_mnist()\n",
        "\n",
        "  with open(save_file, 'rb') as f:\n",
        "    dataset = pickle.load(f)\n",
        "\n",
        "  if normalize:\n",
        "    for key in ('train_img', 'test_img'):\n",
        "      dataset[key] = dataset[key].astype(np.float32)\n",
        "      dataset[key] /= 255.0\n",
        "\n",
        "  if one_hot_label:\n",
        "    dataset['train_label'] = _change_one_hot_label(dataset['train_label'])\n",
        "    dataset['test_label'] = _change_one_hot_label(dataset['test_label'])\n",
        "\n",
        "  if not flatten:\n",
        "    for key in ('train_img', 'test_img'):\n",
        "      dataset[key] = dataset[key].reshape(-1, 1, 28, 28)\n",
        "\n",
        "  return (dataset['train_img'], dataset['train_label']), (dataset['test_img'], dataset['test_label'])\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  init_mnist()"
      ],
      "metadata": {
        "id": "3hxpvxwjO9XH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "733d2b1f-69a4-4f3f-9bee-def2db222a94"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting train-images-idx3-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting train-labels-idx1-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting t10k-images-idx3-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting t10k-labels-idx1-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Creating pickle file ...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 激活层对象"
      ],
      "metadata": {
        "id": "0xt2FjP_ABFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Relu:\n",
        "  def __init__(self):\n",
        "    self.mask = None\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.mask = (x <= 0)\n",
        "    out = x.copy()\n",
        "    out[self.mask] = 0\n",
        "\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    dout[self.mask] = 0\n",
        "    dx = dout\n",
        "\n",
        "    return dx\n",
        "\n",
        "\n",
        "class Sigmoid:\n",
        "  def __init__(self):\n",
        "    self.out = None\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = sigmoid(x)\n",
        "    self.out = out\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    dx = dout * (1.0 - self.out) * self.out\n",
        "\n",
        "    return dx\n",
        "\n",
        "\n",
        "class Affine:\n",
        "  def __init__(self, W, b):\n",
        "    self.W =W\n",
        "    self.b = b\n",
        "\n",
        "    self.x = None\n",
        "    self.original_x_shape = None\n",
        "    # 权重和偏置参数的导数\n",
        "    self.dW = None\n",
        "    self.db = None\n",
        "\n",
        "  def forward(self, x):\n",
        "    # 对应张量\n",
        "    self.original_x_shape = x.shape\n",
        "    x = x.reshape(x.shape[0], -1)\n",
        "    self.x = x\n",
        "\n",
        "    out = np.dot(self.x, self.W) + self.b\n",
        "\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    dx = np.dot(dout, self.W.T)\n",
        "    self.dW = np.dot(self.x.T, dout)\n",
        "    self.db = np.sum(dout, axis=0)\n",
        "\n",
        "    dx = dx.reshape(*self.original_x_shape)  # 还原输入数据的形状（对应张量）\n",
        "    return dx\n",
        "\n",
        "\n",
        "class SoftmaxWithLoss:\n",
        "  def __init__(self):\n",
        "    self.loss = None\n",
        "    self.y = None # softmax的输出\n",
        "    self.t = None # 监督数据\n",
        "\n",
        "  def forward(self, x, t):\n",
        "    self.t = t\n",
        "    self.y = softmax(x)\n",
        "    self.loss = cross_entropy_error(self.y, self.t)\n",
        "\n",
        "    return self.loss\n",
        "\n",
        "  def backward(self, dout=1):\n",
        "    batch_size = self.t.shape[0]\n",
        "    if self.t.size == self.y.size: # 监督数据是one-hot-vector的情况\n",
        "      dx = (self.y - self.t) / batch_size\n",
        "    else:\n",
        "      dx = self.y.copy()\n",
        "      dx[np.arange(batch_size), self.t] -= 1\n",
        "      dx = dx / batch_size\n",
        "\n",
        "    return dx\n",
        "\n",
        "\n",
        "class Dropout:\n",
        "  \"\"\"\n",
        "  http://arxiv.org/abs/1207.0580\n",
        "  \"\"\"\n",
        "  def __init__(self, dropout_ratio=0.5):\n",
        "    self.dropout_ratio = dropout_ratio\n",
        "    self.mask = None\n",
        "\n",
        "  def forward(self, x, train_flg=True):\n",
        "    if train_flg:\n",
        "      self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
        "      return x * self.mask\n",
        "    else:\n",
        "      return x * (1.0 - self.dropout_ratio)\n",
        "\n",
        "  def backward(self, dout):\n",
        "    return dout * self.mask\n",
        "\n",
        "\n",
        "class BatchNormalization:\n",
        "  \"\"\"\n",
        "  http://arxiv.org/abs/1502.03167\n",
        "  \"\"\"\n",
        "  def __init__(self, gamma, beta, momentum=0.9, running_mean=None, running_var=None):\n",
        "    self.gamma = gamma\n",
        "    self.beta = beta\n",
        "    self.momentum = momentum\n",
        "    self.input_shape = None # Conv层的情况下为4维，全连接层的情况下为2维\n",
        "\n",
        "    # 测试时使用的平均值和方差\n",
        "    self.running_mean = running_mean\n",
        "    self.running_var = running_var\n",
        "\n",
        "    # backward时使用的中间数据\n",
        "    self.batch_size = None\n",
        "    self.xc = None\n",
        "    self.std = None\n",
        "    self.dgamma = None\n",
        "    self.dbeta = None\n",
        "\n",
        "  def forward(self, x, train_flg=True):\n",
        "    self.input_shape = x.shape\n",
        "    if x.ndim != 2:\n",
        "      N, C, H, W = x.shape\n",
        "      x = x.reshape(N, -1)\n",
        "\n",
        "    out = self.__forward(x, train_flg)\n",
        "\n",
        "    return out.reshape(*self.input_shape)\n",
        "\n",
        "  def __forward(self, x, train_flg):\n",
        "    if self.running_mean is None:\n",
        "      N, D = x.shape\n",
        "      self.running_mean = np.zeros(D)\n",
        "      self.running_var = np.zeros(D)\n",
        "\n",
        "    if train_flg:\n",
        "      mu = x.mean(axis=0)\n",
        "      xc = x - mu\n",
        "      var = np.mean(xc**2, axis=0)\n",
        "      std = np.sqrt(var + 10e-7)\n",
        "      xn = xc / std\n",
        "\n",
        "      self.batch_size = x.shape[0]\n",
        "      self.xc = xc\n",
        "      self.xn = xn\n",
        "      self.std = std\n",
        "      self.running_mean = self.momentum * self.running_mean + (1-self.momentum) * mu\n",
        "      self.running_var = self.momentum * self.running_var + (1-self.momentum) * var\n",
        "    else:\n",
        "      xc = x - self.running_mean\n",
        "      xn = xc / ((np.sqrt(self.running_var + 10e-7)))\n",
        "\n",
        "    out = self.gamma * xn + self.beta\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    if dout.ndim != 2:\n",
        "      N, C, H, W = dout.shape\n",
        "      dout = dout.reshape(N, -1)\n",
        "\n",
        "    dx = self.__backward(dout)\n",
        "\n",
        "    dx = dx.reshape(*self.input_shape)\n",
        "    return dx\n",
        "\n",
        "  def __backward(self, dout):\n",
        "    dbeta = dout.sum(axis=0)\n",
        "    dgamma = np.sum(self.xn * dout, axis=0)\n",
        "    dxn = self.gamma * dout\n",
        "    dxc = dxn / self.std\n",
        "    dstd = -np.sum((dxn * self.xc) / (self.std * self.std), axis=0)\n",
        "    dvar = 0.5 * dstd / self.std\n",
        "    dxc += (2.0 / self.batch_size) * self.xc * dvar\n",
        "    dmu = np.sum(dxc, axis=0)\n",
        "    dx = dxc - dmu / self.batch_size\n",
        "\n",
        "    self.dgamma = dgamma\n",
        "    self.dbeta = dbeta\n",
        "\n",
        "    return dx\n",
        "\n",
        "\n",
        "class Convolution:\n",
        "  def __init__(self, W, b, stride=1, pad=0):\n",
        "    self.W = W\n",
        "    self.b = b\n",
        "    self.stride = stride\n",
        "    self.pad = pad\n",
        "\n",
        "    # 中间数据（backward时使用）\n",
        "    self.x = None\n",
        "    self.col = None\n",
        "    self.col_W = None\n",
        "\n",
        "    # 权重和偏置参数的梯度\n",
        "    self.dW = None\n",
        "    self.db = None\n",
        "\n",
        "  def forward(self, x):\n",
        "    FN, C, FH, FW = self.W.shape\n",
        "    N, C, H, W = x.shape\n",
        "    out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n",
        "    out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n",
        "\n",
        "    col = im2col(x, FH, FW, self.stride, self.pad)\n",
        "    col_W = self.W.reshape(FN, -1).T\n",
        "\n",
        "    out = np.dot(col, col_W) + self.b\n",
        "    out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
        "\n",
        "    self.x = x\n",
        "    self.col = col\n",
        "    self.col_W = col_W\n",
        "\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    FN, C, FH, FW = self.W.shape\n",
        "    dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
        "\n",
        "    self.db = np.sum(dout, axis=0)\n",
        "    self.dW = np.dot(self.col.T, dout)\n",
        "    self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
        "\n",
        "    dcol = np.dot(dout, self.col_W.T)\n",
        "    dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
        "\n",
        "    return dx\n",
        "\n",
        "\n",
        "class Pooling:\n",
        "  def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
        "    self.pool_h = pool_h\n",
        "    self.pool_w = pool_w\n",
        "    self.stride = stride\n",
        "    self.pad = pad\n",
        "\n",
        "    self.x = None\n",
        "    self.arg_max = None\n",
        "\n",
        "  def forward(self, x):\n",
        "    N, C, H, W = x.shape\n",
        "    out_h = int(1 + (H - self.pool_h) / self.stride)\n",
        "    out_w = int(1 + (W - self.pool_w) / self.stride)\n",
        "\n",
        "    col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "    col = col.reshape(-1, self.pool_h*self.pool_w)\n",
        "\n",
        "    arg_max = np.argmax(col, axis=1)\n",
        "    out = np.max(col, axis=1)\n",
        "    out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
        "\n",
        "    self.x = x\n",
        "    self.arg_max = arg_max\n",
        "\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    dout = dout.transpose(0, 2, 3, 1)\n",
        "\n",
        "    pool_size = self.pool_h * self.pool_w\n",
        "    dmax = np.zeros((dout.size, pool_size))\n",
        "    dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
        "    dmax = dmax.reshape(dout.shape + (pool_size,))\n",
        "\n",
        "    dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
        "    dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "\n",
        "    return dx\n"
      ],
      "metadata": {
        "id": "_3Ni7gTAEBB4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 梯度方法"
      ],
      "metadata": {
        "id": "GOt2e5IjOMBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import numpy as np\n",
        "\n",
        "def _numerical_gradient_1d(f, x):\n",
        "  h = 1e-4 # 0.0001\n",
        "  grad = np.zeros_like(x)\n",
        "\n",
        "  for idx in range(x.size):\n",
        "    tmp_val = x[idx]\n",
        "    x[idx] = float(tmp_val) + h\n",
        "    fxh1 = f(x) # f(x+h)\n",
        "\n",
        "    x[idx] = tmp_val - h\n",
        "    fxh2 = f(x) # f(x-h)\n",
        "    grad[idx] = (fxh1 - fxh2) / (2*h)\n",
        "\n",
        "    x[idx] = tmp_val # 还原值\n",
        "\n",
        "  return grad\n",
        "\n",
        "\n",
        "def numerical_gradient_2d(f, X):\n",
        "  if X.ndim == 1:\n",
        "    return _numerical_gradient_1d(f, X)\n",
        "  else:\n",
        "    grad = np.zeros_like(X)\n",
        "\n",
        "    for idx, x in enumerate(X):\n",
        "      grad[idx] = _numerical_gradient_1d(f, x)\n",
        "\n",
        "    return grad\n",
        "\n",
        "\n",
        "def numerical_gradient(f, x):\n",
        "  h = 1e-4 # 0.0001\n",
        "  grad = np.zeros_like(x)\n",
        "\n",
        "  it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
        "  while not it.finished:\n",
        "    idx = it.multi_index\n",
        "    tmp_val = x[idx]\n",
        "    x[idx] = float(tmp_val) + h\n",
        "    fxh1 = f(x) # f(x+h)\n",
        "\n",
        "    x[idx] = tmp_val - h\n",
        "    fxh2 = f(x) # f(x-h)\n",
        "    grad[idx] = (fxh1 - fxh2) / (2*h)\n",
        "\n",
        "    x[idx] = tmp_val # 还原值\n",
        "    it.iternext()\n",
        "\n",
        "  return grad"
      ],
      "metadata": {
        "id": "faQL25ecOWCt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 优化器"
      ],
      "metadata": {
        "id": "NtkXL5gQPw-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import numpy as np\n",
        "\n",
        "class SGD:\n",
        "\n",
        "  \"\"\"随机梯度下降法（Stochastic Gradient Descent）\"\"\"\n",
        "\n",
        "  def __init__(self, lr=0.01):\n",
        "    self.lr = lr\n",
        "\n",
        "  def update(self, params, grads):\n",
        "    for key in params.keys():\n",
        "      params[key] -= self.lr * grads[key]\n",
        "\n",
        "\n",
        "class Momentum:\n",
        "\n",
        "  \"\"\"Momentum SGD\"\"\"\n",
        "\n",
        "  def __init__(self, lr=0.01, momentum=0.9):\n",
        "    self.lr = lr\n",
        "    self.momentum = momentum\n",
        "    self.v = None\n",
        "\n",
        "  def update(self, params, grads):\n",
        "    if self.v is None:\n",
        "      self.v = {}\n",
        "      for key, val in params.items():\n",
        "        self.v[key] = np.zeros_like(val)\n",
        "\n",
        "    for key in params.keys():\n",
        "      self.v[key] = self.momentum*self.v[key] - self.lr*grads[key]\n",
        "      params[key] += self.v[key]\n",
        "\n",
        "\n",
        "class Nesterov:\n",
        "\n",
        "  \"\"\"Nesterov's Accelerated Gradient (http://arxiv.org/abs/1212.0901)\"\"\"\n",
        "\n",
        "  def __init__(self, lr=0.01, momentum=0.9):\n",
        "    self.lr = lr\n",
        "    self.momentum = momentum\n",
        "    self.v = None\n",
        "\n",
        "  def update(self, params, grads):\n",
        "    if self.v is None:\n",
        "      self.v = {}\n",
        "      for key, val in params.items():\n",
        "        self.v[key] = np.zeros_like(val)\n",
        "\n",
        "    for key in params.keys():\n",
        "      self.v[key] *= self.momentum\n",
        "      self.v[key] -= self.lr * grads[key]\n",
        "      params[key] += self.momentum * self.momentum * self.v[key]\n",
        "      params[key] -= (1 + self.momentum) * self.lr * grads[key]\n",
        "\n",
        "\n",
        "class AdaGrad:\n",
        "\n",
        "  \"\"\"AdaGrad\"\"\"\n",
        "\n",
        "  def __init__(self, lr=0.01):\n",
        "    self.lr = lr\n",
        "    self.h = None\n",
        "\n",
        "  def update(self, params, grads):\n",
        "    if self.h is None:\n",
        "      self.h = {}\n",
        "      for key, val in params.items():\n",
        "        self.h[key] = np.zeros_like(val)\n",
        "\n",
        "    for key in params.keys():\n",
        "      self.h[key] += grads[key] * grads[key]\n",
        "      params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n",
        "\n",
        "\n",
        "class RMSprop:\n",
        "\n",
        "  \"\"\"RMSprop\"\"\"\n",
        "\n",
        "  def __init__(self, lr=0.01, decay_rate = 0.99):\n",
        "    self.lr = lr\n",
        "    self.decay_rate = decay_rate\n",
        "    self.h = None\n",
        "\n",
        "  def update(self, params, grads):\n",
        "    if self.h is None:\n",
        "      self.h = {}\n",
        "      for key, val in params.items():\n",
        "        self.h[key] = np.zeros_like(val)\n",
        "\n",
        "    for key in params.keys():\n",
        "      self.h[key] *= self.decay_rate\n",
        "      self.h[key] += (1 - self.decay_rate) * grads[key] * grads[key]\n",
        "      params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n",
        "\n",
        "\n",
        "class Adam:\n",
        "\n",
        "  \"\"\"Adam (http://arxiv.org/abs/1412.6980v8)\"\"\"\n",
        "\n",
        "  def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
        "    self.lr = lr\n",
        "    self.beta1 = beta1\n",
        "    self.beta2 = beta2\n",
        "    self.iter = 0\n",
        "    self.m = None\n",
        "    self.v = None\n",
        "\n",
        "  def update(self, params, grads):\n",
        "    if self.m is None:\n",
        "      self.m, self.v = {}, {}\n",
        "      for key, val in params.items():\n",
        "        self.m[key] = np.zeros_like(val)\n",
        "        self.v[key] = np.zeros_like(val)\n",
        "\n",
        "    self.iter += 1\n",
        "    lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)\n",
        "\n",
        "    for key in params.keys():\n",
        "      #self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]\n",
        "      #self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)\n",
        "      self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
        "      self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n",
        "\n",
        "      params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)\n",
        "\n",
        "      #unbias_m += (1 - self.beta1) * (grads[key] - self.m[key]) # correct bias\n",
        "      #unbisa_b += (1 - self.beta2) * (grads[key]*grads[key] - self.v[key]) # correct bias\n",
        "      #params[key] += self.lr * unbias_m / (np.sqrt(unbisa_b) + 1e-7)\n"
      ],
      "metadata": {
        "id": "IMUhp_nmPwxf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 创建网络对象"
      ],
      "metadata": {
        "id": "QhZemmxP_G8-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1Sg7GtR8-4f0"
      },
      "outputs": [],
      "source": [
        "# coding: utf-8\n",
        "import sys, os\n",
        "sys.path.append(os.pardir)  # 为了导入父目录的文件而进行的设定\n",
        "import pickle\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "class SimpleConvNet:\n",
        "  \"\"\"简单的ConvNet\n",
        "\n",
        "  conv - relu - pool - affine - relu - affine - softmax\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  input_size : 输入大小（MNIST的情况下为784）\n",
        "  hidden_size_list : 隐藏层的神经元数量的列表（e.g. [100, 100, 100]）\n",
        "  output_size : 输出大小（MNIST的情况下为10）\n",
        "  activation : 'relu' or 'sigmoid'\n",
        "  weight_init_std : 指定权重的标准差（e.g. 0.01）\n",
        "    指定'relu'或'he'的情况下设定“He的初始值”\n",
        "    指定'sigmoid'或'xavier'的情况下设定“Xavier的初始值”\n",
        "  \"\"\"\n",
        "  def __init__(self, input_dim=(1, 28, 28),\n",
        "         conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
        "         hidden_size=100, output_size=10, weight_init_std=0.01):\n",
        "    filter_num = conv_param['filter_num']\n",
        "    filter_size = conv_param['filter_size']\n",
        "    filter_pad = conv_param['pad']\n",
        "    filter_stride = conv_param['stride']\n",
        "    input_size = input_dim[1]\n",
        "    conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
        "    pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
        "\n",
        "    # 初始化权重\n",
        "    self.params = {}\n",
        "    self.params['W1'] = weight_init_std * \\\n",
        "              np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
        "    self.params['b1'] = np.zeros(filter_num)\n",
        "    self.params['W2'] = weight_init_std * \\\n",
        "              np.random.randn(pool_output_size, hidden_size)\n",
        "    self.params['b2'] = np.zeros(hidden_size)\n",
        "    self.params['W3'] = weight_init_std * \\\n",
        "              np.random.randn(hidden_size, output_size)\n",
        "    self.params['b3'] = np.zeros(output_size)\n",
        "\n",
        "    # 生成层\n",
        "    self.layers = OrderedDict()\n",
        "    self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
        "                       conv_param['stride'], conv_param['pad'])\n",
        "    self.layers['Relu1'] = Relu()\n",
        "    self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
        "    self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
        "    self.layers['Relu2'] = Relu()\n",
        "    self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
        "\n",
        "    self.last_layer = SoftmaxWithLoss()\n",
        "\n",
        "  def predict(self, x):\n",
        "    for layer in self.layers.values():\n",
        "      x = layer.forward(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "  def loss(self, x, t):\n",
        "    \"\"\"求损失函数\n",
        "    参数x是输入数据、t是教师标签\n",
        "    \"\"\"\n",
        "    y = self.predict(x)\n",
        "    return self.last_layer.forward(y, t)\n",
        "\n",
        "  def accuracy(self, x, t, batch_size=100):\n",
        "    if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
        "\n",
        "    acc = 0.0\n",
        "\n",
        "    for i in range(int(x.shape[0] / batch_size)):\n",
        "      tx = x[i*batch_size:(i+1)*batch_size]\n",
        "      tt = t[i*batch_size:(i+1)*batch_size]\n",
        "      y = self.predict(tx)\n",
        "      y = np.argmax(y, axis=1)\n",
        "      acc += np.sum(y == tt)\n",
        "\n",
        "    return acc / x.shape[0]\n",
        "\n",
        "  def numerical_gradient(self, x, t):\n",
        "    \"\"\"求梯度（数值微分）\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : 输入数据\n",
        "    t : 教师标签\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    具有各层的梯度的字典变量\n",
        "      grads['W1']、grads['W2']、...是各层的权重\n",
        "      grads['b1']、grads['b2']、...是各层的偏置\n",
        "    \"\"\"\n",
        "    loss_w = lambda w: self.loss(x, t)\n",
        "\n",
        "    grads = {}\n",
        "    for idx in (1, 2, 3):\n",
        "      grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
        "      grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
        "\n",
        "    return grads\n",
        "\n",
        "  def gradient(self, x, t):\n",
        "    \"\"\"求梯度（误差反向传播法）\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : 输入数据\n",
        "    t : 教师标签\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    具有各层的梯度的字典变量\n",
        "      grads['W1']、grads['W2']、...是各层的权重\n",
        "      grads['b1']、grads['b2']、...是各层的偏置\n",
        "    \"\"\"\n",
        "    # forward\n",
        "    self.loss(x, t)\n",
        "\n",
        "    # backward\n",
        "    dout = 1\n",
        "    dout = self.last_layer.backward(dout)\n",
        "\n",
        "    layers = list(self.layers.values())\n",
        "    layers.reverse()\n",
        "    for layer in layers:\n",
        "      dout = layer.backward(dout)\n",
        "\n",
        "    # 设定\n",
        "    grads = {}\n",
        "    grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
        "    grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
        "    grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
        "\n",
        "    return grads\n",
        "\n",
        "  def save_params(self, file_name=\"params.pkl\"):\n",
        "    params = {}\n",
        "    for key, val in self.params.items():\n",
        "      params[key] = val\n",
        "    with open(file_name, 'wb') as f:\n",
        "      pickle.dump(params, f)\n",
        "\n",
        "  def load_params(self, file_name=\"params.pkl\"):\n",
        "    with open(file_name, 'rb') as f:\n",
        "      params = pickle.load(f)\n",
        "    for key, val in params.items():\n",
        "      self.params[key] = val\n",
        "\n",
        "    for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
        "      self.layers[key].W = self.params['W' + str(i+1)]\n",
        "      self.layers[key].b = self.params['b' + str(i+1)]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 创建训练类"
      ],
      "metadata": {
        "id": "RUdiL_YO_FyY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import sys, os\n",
        "sys.path.append(os.pardir)  # 为了导入父目录的文件而进行的设定\n",
        "import numpy as np\n",
        "\n",
        "class Trainer:\n",
        "  \"\"\"进行神经网络的训练的类\n",
        "  \"\"\"\n",
        "  def __init__(self, network, x_train, t_train, x_test, t_test,\n",
        "         epochs=20, mini_batch_size=100,\n",
        "         optimizer='SGD', optimizer_param={'lr':0.01},\n",
        "         evaluate_sample_num_per_epoch=None, verbose=True):\n",
        "    self.network = network\n",
        "    self.verbose = verbose\n",
        "    self.x_train = x_train\n",
        "    self.t_train = t_train\n",
        "    self.x_test = x_test\n",
        "    self.t_test = t_test\n",
        "    self.epochs = epochs\n",
        "    self.batch_size = mini_batch_size\n",
        "    self.evaluate_sample_num_per_epoch = evaluate_sample_num_per_epoch\n",
        "\n",
        "    # optimzer\n",
        "    optimizer_class_dict = {'sgd':SGD, 'momentum':Momentum, 'nesterov':Nesterov,\n",
        "                'adagrad':AdaGrad, 'rmsprpo':RMSprop, 'adam':Adam}\n",
        "    self.optimizer = optimizer_class_dict[optimizer.lower()](**optimizer_param)\n",
        "\n",
        "    self.train_size = x_train.shape[0]\n",
        "    self.iter_per_epoch = max(self.train_size / mini_batch_size, 1)\n",
        "    self.max_iter = int(epochs * self.iter_per_epoch)\n",
        "    self.current_iter = 0\n",
        "    self.current_epoch = 0\n",
        "\n",
        "    self.train_loss_list = []\n",
        "    self.train_acc_list = []\n",
        "    self.test_acc_list = []\n",
        "\n",
        "  def train_step(self):\n",
        "    batch_mask = np.random.choice(self.train_size, self.batch_size)\n",
        "    x_batch = self.x_train[batch_mask]\n",
        "    t_batch = self.t_train[batch_mask]\n",
        "\n",
        "    grads = self.network.gradient(x_batch, t_batch)\n",
        "    self.optimizer.update(self.network.params, grads)\n",
        "\n",
        "    loss = self.network.loss(x_batch, t_batch)\n",
        "    self.train_loss_list.append(loss)\n",
        "    if self.verbose: print(\"train loss:\" + str(loss))\n",
        "\n",
        "    if self.current_iter % self.iter_per_epoch == 0:\n",
        "      self.current_epoch += 1\n",
        "\n",
        "      x_train_sample, t_train_sample = self.x_train, self.t_train\n",
        "      x_test_sample, t_test_sample = self.x_test, self.t_test\n",
        "      if not self.evaluate_sample_num_per_epoch is None:\n",
        "        t = self.evaluate_sample_num_per_epoch\n",
        "        x_train_sample, t_train_sample = self.x_train[:t], self.t_train[:t]\n",
        "        x_test_sample, t_test_sample = self.x_test[:t], self.t_test[:t]\n",
        "\n",
        "      train_acc = self.network.accuracy(x_train_sample, t_train_sample)\n",
        "      test_acc = self.network.accuracy(x_test_sample, t_test_sample)\n",
        "      self.train_acc_list.append(train_acc)\n",
        "      self.test_acc_list.append(test_acc)\n",
        "\n",
        "      print(\"=== epoch:\" + str(self.current_epoch) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc) + \" ===\")\n",
        "    self.current_iter += 1\n",
        "\n",
        "  def train(self):\n",
        "    for i in range(self.max_iter):\n",
        "      self.train_step()\n",
        "\n",
        "    test_acc = self.network.accuracy(self.x_test, self.t_test)\n",
        "\n",
        "    if self.verbose:\n",
        "      print(\"=============== Final Test Accuracy ===============\")\n",
        "      print(\"test acc:\" + str(test_acc))\n",
        "\n"
      ],
      "metadata": {
        "id": "KkAyJ-OrPfBf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 执行训练"
      ],
      "metadata": {
        "id": "1up12xMhvE67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import sys, os\n",
        "sys.path.append(os.pardir)  # 为了导入父目录的文件而进行的设定\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 读入数据\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
        "\n",
        "# 处理花费时间较长的情况下减少数据\n",
        "#x_train, t_train = x_train[:5000], t_train[:5000]\n",
        "#x_test, t_test = x_test[:1000], t_test[:1000]\n",
        "\n",
        "max_epochs = 10\n",
        "\n",
        "network = SimpleConvNet(input_dim=(1,28,28),\n",
        "            conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
        "            hidden_size=100, output_size=10, weight_init_std=0.01)\n",
        "\n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
        "          epochs=max_epochs, mini_batch_size=100,\n",
        "          optimizer='Adam', optimizer_param={'lr': 0.001},\n",
        "          evaluate_sample_num_per_epoch=1000, verbose=False)\n",
        "trainer.train()\n",
        "\n",
        "# 保存参数\n",
        "network.save_params(\"params.pkl\")\n",
        "print(\"Saved Network Parameters!\")\n",
        "\n",
        "# 绘制图形\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(max_epochs)\n",
        "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
        "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vnrTOx_vEql",
        "outputId": "986d51d4-8d00-46d9-c434-e0f63fab0365"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== epoch:1, train acc:0.187, test acc:0.19 ===\n",
            "=== epoch:2, train acc:0.956, test acc:0.965 ===\n",
            "=== epoch:3, train acc:0.973, test acc:0.98 ===\n",
            "=== epoch:4, train acc:0.981, test acc:0.978 ===\n",
            "=== epoch:5, train acc:0.989, test acc:0.986 ===\n",
            "=== epoch:6, train acc:0.989, test acc:0.987 ===\n",
            "=== epoch:7, train acc:0.99, test acc:0.986 ===\n",
            "=== epoch:8, train acc:0.992, test acc:0.984 ===\n",
            "=== epoch:9, train acc:0.993, test acc:0.989 ===\n",
            "=== epoch:10, train acc:0.991, test acc:0.988 ===\n",
            "Saved Network Parameters!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}