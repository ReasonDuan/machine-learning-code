{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZOpu/CM4WkHZNTCDuw8no",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ReasonDuan/machine-learning-code/blob/main/network_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 前置函数\n"
      ],
      "metadata": {
        "id": "APqEo5VmHUzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "# 激活函数\n",
        "def sigmoid(x):\n",
        " return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_grad(x):\n",
        "    return (1.0 - sigmoid(x)) * sigmoid(x)\n",
        "\n",
        "# 最大值\n",
        "def softmax(a):\n",
        " exp_a = np.exp(a)\n",
        " sum_exp_a = np.sum(exp_a)\n",
        " y = exp_a / sum_exp_a\n",
        " return y\n",
        "\n",
        "\n",
        "# 函数f在x处的梯度计算\n",
        "def numerical_gradient(f, x):\n",
        " h = 1e-4 # 0.0001\n",
        " grad = np.zeros_like(x) # 生成和x形状相同的数组\n",
        " for idx in range(x.size):\n",
        "  tmp_val = x[idx]\n",
        "  # f(x+h)的计算\n",
        "  x[idx] = tmp_val + h\n",
        "  fxh1 = f(x)\n",
        "  # f(x-h)的计算\n",
        "  x[idx] = tmp_val - h\n",
        "  fxh2 = f(x)\n",
        "  grad[idx] = (fxh1 - fxh2) / (2*h)\n",
        "  x[idx] = tmp_val # 还原值\n",
        " return grad\n",
        "\n",
        "# 交叉熵，监督数据为one-hot模式\n",
        "def cross_entropy_error(y, t):\n",
        " if y.ndim == 1:\n",
        "  t = t.reshape(1, t.size)\n",
        "  y = y.reshape(1, y.size)\n",
        " batch_size = y.shape[0]\n",
        " return -np.sum(t * np.log(y + 1e-7)) / batch_size\n",
        "\n",
        "\n",
        "# 交叉熵, 监督数据为指定值 2，8\n",
        "def cross_entropy_error_02(y, t):\n",
        " if y.ndim == 1:\n",
        "  t = t.reshape(1, t.size)\n",
        "  y = y.reshape(1, y.size)\n",
        " batch_size = y.shape[0]\n",
        " return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
        "\n",
        "\n",
        "def change_one_hot_label(X):\n",
        "  T = np.zeros((X.size, 10))\n",
        "  for idx, row in enumerate(T):\n",
        "      row[X[idx]] = 1\n",
        "\n",
        "  return T\n"
      ],
      "metadata": {
        "id": "P-aOkmQ6HV1V"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 创建神经网络类\n",
        "\n",
        "创建一个2层神经网络的类"
      ],
      "metadata": {
        "id": "1SEDGeDmfvBU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "E6DH0onefFIM"
      },
      "outputs": [],
      "source": [
        "\n",
        "import sys, os\n",
        "sys.path.append(os.pardir)\n",
        "\n",
        "class TwoLayerNet:\n",
        "  def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
        "    self.params = {}\n",
        "    self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
        "    self.params['b1'] = np.zeros(hidden_size)\n",
        "    self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
        "    self.params['b2'] = np.zeros(output_size)\n",
        "\n",
        "  # 预测\n",
        "  def predict(self, x):\n",
        "    W1, W2 = self.params['W1'], self.params['W2']\n",
        "    b1, b2 = self.params['b1'], self.params['b2']\n",
        "\n",
        "    a1 = np.dot(x, W1) + b1\n",
        "    z1 = sigmoid(a1)\n",
        "    a2 = np.dot(z1, W2) + b2\n",
        "    y = softmax(a2)\n",
        "    return y\n",
        "\n",
        "  # 损失率， x: 输入数据, t: 监督数据\n",
        "  def loss(self, x, t):\n",
        "    y = self.predict(x)\n",
        "    return cross_entropy_error(y, t)\n",
        "\n",
        "  # 准确率\n",
        "  def accuracy(self, x, t):\n",
        "    y = self.predict(x)\n",
        "    y = np.argmax(y, axis=1)\n",
        "    t = np.argmax(t, axis=1)\n",
        "\n",
        "    accuracy = np.sum(y == t) / float(x.shape[0])\n",
        "    return accuracy\n",
        "\n",
        "  # 权重参数相对损失函数的梯度计算\n",
        "  def numerical_gradient_full(self, x, t):\n",
        "    loss_W = lambda W: self.loss(x, t)\n",
        "\n",
        "    grads = {}\n",
        "    grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
        "    grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
        "    grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
        "    grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
        "\n",
        "    return grads\n",
        "\n",
        "  def gradient(self, x, t):\n",
        "    W1, W2 = self.params['W1'], self.params['W2']\n",
        "    b1, b2 = self.params['b1'], self.params['b2']\n",
        "    grads = {}\n",
        "\n",
        "    batch_num = x.shape[0]\n",
        "\n",
        "    # forward\n",
        "    a1 = np.dot(x, W1) + b1\n",
        "    z1 = sigmoid(a1)\n",
        "    a2 = np.dot(z1, W2) + b2\n",
        "    y = softmax(a2)\n",
        "\n",
        "    # backward\n",
        "    dy = (y - t) / batch_num\n",
        "    grads['W2'] = np.dot(z1.T, dy)\n",
        "    grads['b2'] = np.sum(dy, axis=0)\n",
        "\n",
        "    da1 = np.dot(dy, W2.T)\n",
        "    dz1 = sigmoid_grad(a1) * da1\n",
        "    grads['W1'] = np.dot(x.T, dz1)\n",
        "    grads['b1'] = np.sum(dz1, axis=0)\n",
        "\n",
        "    return grads"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pxfN5pyDHSVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 拉取数据\n",
        "获取训练数据，并进行初始化\n",
        "**加粗文字**"
      ],
      "metadata": {
        "id": "swdLUgS5k40P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tensorflow\n",
        "import tensorflow as tf\n",
        "\n",
        "# 加载 MNIST 数据集\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, t_train), (x_test, t_test) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.reshape(x_train.shape[0], -1)\n",
        "x_test = x_test.reshape(x_test.shape[0], -1)\n",
        "t_train = change_one_hot_label(t_train)\n",
        "t_test = change_one_hot_label(t_test)\n",
        "\n",
        "\n",
        "# (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_laobel = True)\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "# 超参数\n",
        "iters_num = 10000\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "# 平均每个epoch的重复次数\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n"
      ],
      "metadata": {
        "id": "wYdnNZGklCwQ"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 开始训练"
      ],
      "metadata": {
        "id": "y8FjJPXllUV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "for i in range(iters_num):\n",
        "  batch_mask = np.random.choice(train_size, batch_size)\n",
        "  x_batch = x_train[batch_mask]\n",
        "  t_batch = t_train[batch_mask]\n",
        "\n",
        "  grad = network.gradient(x_batch, t_batch)\n",
        "\n",
        "  for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "    network.params[key] -= learning_rate * grad[key]\n",
        "\n",
        "  loss = network.loss(x_batch, t_batch)\n",
        "  train_loss_list.append(loss)\n",
        "  # 计算每个epoch的识别精度\n",
        "  if i % iter_per_epoch == 0:\n",
        "    train_acc = network.accuracy(x_train, t_train)\n",
        "    test_acc = network.accuracy(x_test, t_test)\n",
        "    train_acc_list.append(train_acc)\n",
        "    test_acc_list.append(test_acc)\n",
        "    print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# 绘制图形\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(len(train_acc_list))\n",
        "plt.plot(x, train_acc_list, label='train acc')\n",
        "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B53n-KellXeQ",
        "outputId": "2d0f86ce-f62e-415f-a5d7-fc5162b3d2c3"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "train acc, test acc | 0.1243, 0.1284\n",
            "train acc, test acc | 0.11236666666666667, 0.1135\n",
            "train acc, test acc | 0.11236666666666667, 0.1135\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-41-a8bce8d1d44b>:13: RuntimeWarning: overflow encountered in exp\n",
            "  exp_a = np.exp(a)\n",
            "<ipython-input-41-a8bce8d1d44b>:15: RuntimeWarning: invalid value encountered in divide\n",
            "  y = exp_a / sum_exp_a\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train acc, test acc | 0.09871666666666666, 0.098\n",
            "train acc, test acc | 0.09871666666666666, 0.098\n",
            "train acc, test acc | 0.09871666666666666, 0.098\n",
            "train acc, test acc | 0.09871666666666666, 0.098\n",
            "train acc, test acc | 0.09871666666666666, 0.098\n",
            "train acc, test acc | 0.09871666666666666, 0.098\n",
            "train acc, test acc | 0.09871666666666666, 0.098\n",
            "train acc, test acc | 0.09871666666666666, 0.098\n",
            "train acc, test acc | 0.09871666666666666, 0.098\n",
            "train acc, test acc | 0.09871666666666666, 0.098\n",
            "train acc, test acc | 0.09871666666666666, 0.098\n",
            "train acc, test acc | 0.09871666666666666, 0.098\n",
            "train acc, test acc | 0.09871666666666666, 0.098\n",
            "train acc, test acc | 0.09871666666666666, 0.098\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Relu 层"
      ],
      "metadata": {
        "id": "hz3uceNsmKco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Relu:\n",
        "  def __init__(self):\n",
        "    self.mask = None\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.mask = (x <= 0)\n",
        "    out = x.copy()\n",
        "    out[self.mask] = 0\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    dout[self.mask] = 0\n",
        "    dx = dout\n",
        "    return dx\n",
        "\n",
        "relu = Relu()\n",
        "x = np.array([[1.0, -0.5], [-2.0, 3.0]])\n",
        "print(relu.forward(x))\n",
        "mask = (x <= 0)\n",
        "print(mask)\n",
        "print(relu.backward(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdBZw3OZHfan",
        "outputId": "47f1b8d5-2b1f-45b5-d6f1-71b2c47bbd1f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 0.]\n",
            " [0. 3.]]\n",
            "[[False  True]\n",
            " [ True False]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sigmoid 层"
      ],
      "metadata": {
        "id": "81gP97CKmDEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "class Sigmoid:\n",
        "  def __init__(self):\n",
        "    self.out = None\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = 1 / (1 + np.exp(-x))\n",
        "    self.out = out\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    dx = dout * (1.0 - self.out) * self.out\n",
        "    return dx\n",
        "\n",
        "sigmoid = Sigmoid()\n",
        "x = np.array([[1.0, -0.5], [-2.0, 3.0]])\n",
        "print(sigmoid.forward(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2drkTc3TmPs4",
        "outputId": "e898bd01-04c9-4d90-ca68-899651af2abf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.73105858 0.37754067]\n",
            " [0.11920292 0.95257413]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Affine 层"
      ],
      "metadata": {
        "id": "sYgYtP4nqk4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Affine:\n",
        "  def __init__(self, W, b):\n",
        "    self.W = W\n",
        "    self.b = b\n",
        "    self.x = None\n",
        "    self.dW = None\n",
        "    self.db = None\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.x = x\n",
        "    out = np.dot(x, self.W) + self.b\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    dx = np.dot(dout, self.W.T)\n",
        "    self.dW = np.dot(self.x.T, dout)\n",
        "    self.db = np.sum(dout, axis=0)\n",
        "    return dx"
      ],
      "metadata": {
        "id": "B6ODi_aTqkdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Softmax-with-Loss 层"
      ],
      "metadata": {
        "id": "lzw2f_qGrE2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SoftmaxWithLoss:\n",
        "  def __init__(self):\n",
        "    self.loss = None\n",
        "    self.y = None\n",
        "    self.t = None\n",
        "\n",
        "  def forward(self, x, t):\n",
        "    self.t = t\n",
        "    self.y = softmax(x)\n",
        "    self.loss = cross_entropy_error(self.y, self.t)\n",
        "    return self.loss\n",
        "\n",
        "  def backward(self, dout=1):\n",
        "    batch_size = self.t.shape[0]\n",
        "    dx = (self.y - self.t) / batch_size\n",
        "    return dx"
      ],
      "metadata": {
        "id": "Dd1u1aRorKN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 打印python版本号\n",
        "import sys\n",
        "print(sys.version)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vX0N3K7FFU6",
        "outputId": "b1cd032d-2c83-4183-bd80-70b41b23272f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "加载数据"
      ],
      "metadata": {
        "id": "f7wAyUw8B2dH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import requests\n",
        "import os.path\n",
        "import gzip\n",
        "import pickle\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# https://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
        "url_base = 'https://ossci-datasets.s3.amazonaws.com/mnist/'  # mirror site\n",
        "key_file = {\n",
        "    'train_img':'train-images-idx3-ubyte.gz',\n",
        "    'train_label':'train-labels-idx1-ubyte.gz',\n",
        "    'test_img':'t10k-images-idx3-ubyte.gz',\n",
        "    'test_label':'t10k-labels-idx1-ubyte.gz'\n",
        "}\n",
        "\n",
        "dataset_dir = './data'\n",
        "save_file = dataset_dir + \"/mnist.pkl\"\n",
        "\n",
        "train_num = 60000\n",
        "test_num = 10000\n",
        "img_dim = (1, 28, 28)\n",
        "img_size = 784\n",
        "\n",
        "\n",
        "def _download(file_name):\n",
        "\n",
        "  # 确保目录存在\n",
        "  os.makedirs(dataset_dir, exist_ok=True)\n",
        "\n",
        "  save_path = os.path.join(dataset_dir, file_name)\n",
        "\n",
        "  # 判断文件是否存在\n",
        "  if os.path.exists(save_path):\n",
        "    return\n",
        "  # 下载文件\n",
        "  response = requests.get(url_base + file_name, stream=True)\n",
        "\n",
        "  # 将内容保存到指定目录下\n",
        "  with open(save_path, 'wb') as file:\n",
        "    for chunk in response.iter_content(chunk_size=1024):\n",
        "      if chunk:\n",
        "        file.write(chunk)\n",
        "\n",
        "  print(f\"文件已下载并保存到: {save_path}\")\n",
        "\n",
        "\n",
        "def download_mnist():\n",
        "  for v in key_file.values():\n",
        "    _download(v)\n",
        "\n",
        "def _load_label(file_name):\n",
        "  file_path = dataset_dir + \"/\" + file_name\n",
        "\n",
        "  print(\"Converting \" + file_name + \" to NumPy Array ...\")\n",
        "  with gzip.open(file_path, 'rb') as f:\n",
        "    labels = np.frombuffer(f.read(), np.uint8, offset=8)\n",
        "  print(\"Done\")\n",
        "\n",
        "  return labels\n",
        "\n",
        "def _load_img(file_name):\n",
        "  file_path = dataset_dir + \"/\" + file_name\n",
        "\n",
        "  print(\"Converting \" + file_name + \" to NumPy Array ...\")\n",
        "  with gzip.open(file_path, 'rb') as f:\n",
        "    data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
        "  data = data.reshape(-1, img_size)\n",
        "  print(\"Done\")\n",
        "\n",
        "  return data\n",
        "\n",
        "def _convert_numpy():\n",
        "  dataset = {}\n",
        "  dataset['train_img'] =  _load_img(key_file['train_img'])\n",
        "  dataset['train_label'] = _load_label(key_file['train_label'])\n",
        "  dataset['test_img'] = _load_img(key_file['test_img'])\n",
        "  dataset['test_label'] = _load_label(key_file['test_label'])\n",
        "\n",
        "  return dataset\n",
        "\n",
        "def init_mnist():\n",
        "  download_mnist()\n",
        "  dataset = _convert_numpy()\n",
        "  print(\"Creating pickle file ...\")\n",
        "  with open(save_file, 'wb') as f:\n",
        "    pickle.dump(dataset, f, -1)\n",
        "  print(\"Done!\")\n",
        "\n",
        "def _change_one_hot_label(X):\n",
        "  T = np.zeros((X.size, 10))\n",
        "  for idx, row in enumerate(T):\n",
        "      row[X[idx]] = 1\n",
        "\n",
        "  return T\n",
        "\n",
        "\n",
        "def load_mnist(normalize=True, flatten=True, one_hot_label=False):\n",
        "  \"\"\"读入MNIST数据集\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  normalize : 将图像的像素值正规化为0.0~1.0\n",
        "  one_hot_label :\n",
        "    one_hot_label为True的情况下，标签作为one-hot数组返回\n",
        "    one-hot数组是指[0,0,1,0,0,0,0,0,0,0]这样的数组\n",
        "  flatten : 是否将图像展开为一维数组\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  (训练图像, 训练标签), (测试图像, 测试标签)\n",
        "  \"\"\"\n",
        "  if not os.path.exists(save_file):\n",
        "    init_mnist()\n",
        "\n",
        "  with open(save_file, 'rb') as f:\n",
        "    dataset = pickle.load(f)\n",
        "\n",
        "  if normalize:\n",
        "    for key in ('train_img', 'test_img'):\n",
        "      dataset[key] = dataset[key].astype(np.float32)\n",
        "      dataset[key] /= 255.0\n",
        "\n",
        "  if one_hot_label:\n",
        "    dataset['train_label'] = _change_one_hot_label(dataset['train_label'])\n",
        "    dataset['test_label'] = _change_one_hot_label(dataset['test_label'])\n",
        "\n",
        "  if not flatten:\n",
        "    for key in ('train_img', 'test_img'):\n",
        "      dataset[key] = dataset[key].reshape(-1, 1, 28, 28)\n",
        "\n",
        "  return (dataset['train_img'], dataset['train_label']), (dataset['test_img'], dataset['test_label'])\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  init_mnist()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foEfAtRjB3vf",
        "outputId": "d2adff32-f0a0-413a-a16b-0be4fd01183d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "文件已下载并保存到: ./data/train-images-idx3-ubyte.gz\n",
            "文件已下载并保存到: ./data/train-labels-idx1-ubyte.gz\n",
            "文件已下载并保存到: ./data/t10k-images-idx3-ubyte.gz\n",
            "文件已下载并保存到: ./data/t10k-labels-idx1-ubyte.gz\n",
            "Converting train-images-idx3-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting train-labels-idx1-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting t10k-images-idx3-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting t10k-labels-idx1-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Creating pickle file ...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "常用方法"
      ],
      "metadata": {
        "id": "jLjok1mQJVFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def identity_function(x):\n",
        "  return x\n",
        "\n",
        "\n",
        "def step_function(x):\n",
        "  return np.array(x > 0, dtype=np.int)\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "def sigmoid_grad(x):\n",
        "  return (1.0 - sigmoid(x)) * sigmoid(x)\n",
        "\n",
        "\n",
        "def relu(x):\n",
        "  return np.maximum(0, x)\n",
        "\n",
        "\n",
        "def relu_grad(x):\n",
        "  grad = np.zeros(x)\n",
        "  grad[x>=0] = 1\n",
        "  return grad\n",
        "\n",
        "\n",
        "def softmax(x):\n",
        "  if x.ndim == 2:\n",
        "    x = x.T\n",
        "    x = x - np.max(x, axis=0)\n",
        "    y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "    return y.T\n",
        "\n",
        "  x = x - np.max(x) # 溢出对策\n",
        "  return np.exp(x) / np.sum(np.exp(x))\n",
        "\n",
        "\n",
        "def mean_squared_error(y, t):\n",
        "  return 0.5 * np.sum((y-t)**2)\n",
        "\n",
        "\n",
        "def cross_entropy_error(y, t):\n",
        "  if y.ndim == 1:\n",
        "    t = t.reshape(1, t.size)\n",
        "    y = y.reshape(1, y.size)\n",
        "\n",
        "  # 监督数据是one-hot-vector的情况下，转换为正确解标签的索引\n",
        "  if t.size == y.size:\n",
        "    t = t.argmax(axis=1)\n",
        "\n",
        "  batch_size = y.shape[0]\n",
        "  return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
        "\n",
        "\n",
        "def softmax_loss(X, t):\n",
        "  y = softmax(X)\n",
        "  return cross_entropy_error(y, t)\n"
      ],
      "metadata": {
        "id": "-WNc6P8CJcDl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "工具"
      ],
      "metadata": {
        "id": "XvjoU8TyJkFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def smooth_curve(x):\n",
        "    \"\"\"用于使损失函数的图形变圆滑\n",
        "\n",
        "    参考：http://glowingpython.blogspot.jp/2012/02/convolution-with-numpy.html\n",
        "    \"\"\"\n",
        "    window_len = 11\n",
        "    s = np.r_[x[window_len-1:0:-1], x, x[-1:-window_len:-1]]\n",
        "    w = np.kaiser(window_len, 2)\n",
        "    y = np.convolve(w/w.sum(), s, mode='valid')\n",
        "    return y[5:len(y)-5]\n",
        "\n",
        "\n",
        "def shuffle_dataset(x, t):\n",
        "    \"\"\"打乱数据集\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : 训练数据\n",
        "    t : 监督数据\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    x, t : 打乱的训练数据和监督数据\n",
        "    \"\"\"\n",
        "    permutation = np.random.permutation(x.shape[0])\n",
        "    x = x[permutation,:] if x.ndim == 2 else x[permutation,:,:,:]\n",
        "    t = t[permutation]\n",
        "\n",
        "    return x, t\n",
        "\n",
        "def conv_output_size(input_size, filter_size, stride=1, pad=0):\n",
        "    return (input_size + 2*pad - filter_size) / stride + 1\n",
        "\n",
        "\n",
        "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
        "    \"\"\"\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_data : 由(数据量, 通道, 高, 长)的4维数组构成的输入数据\n",
        "    filter_h : 滤波器的高\n",
        "    filter_w : 滤波器的长\n",
        "    stride : 步幅\n",
        "    pad : 填充\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    col : 2维数组\n",
        "    \"\"\"\n",
        "    N, C, H, W = input_data.shape\n",
        "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
        "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
        "\n",
        "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
        "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
        "\n",
        "    for y in range(filter_h):\n",
        "        y_max = y + stride*out_h\n",
        "        for x in range(filter_w):\n",
        "            x_max = x + stride*out_w\n",
        "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
        "\n",
        "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
        "    return col\n",
        "\n",
        "\n",
        "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
        "    \"\"\"\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    col :\n",
        "    input_shape : 输入数据的形状（例：(10, 1, 28, 28)）\n",
        "    filter_h :\n",
        "    filter_w\n",
        "    stride\n",
        "    pad\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "\n",
        "    \"\"\"\n",
        "    N, C, H, W = input_shape\n",
        "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
        "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
        "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
        "\n",
        "    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
        "    for y in range(filter_h):\n",
        "        y_max = y + stride*out_h\n",
        "        for x in range(filter_w):\n",
        "            x_max = x + stride*out_w\n",
        "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
        "\n",
        "    return img[:, :, pad:H + pad, pad:W + pad]"
      ],
      "metadata": {
        "id": "zZ8n2TNCJlX3"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "网络层"
      ],
      "metadata": {
        "id": "dE353ABgJrv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Relu:\n",
        "    def __init__(self):\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.mask = (x <= 0)\n",
        "        out = x.copy()\n",
        "        out[self.mask] = 0\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout[self.mask] = 0\n",
        "        dx = dout\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = sigmoid(x)\n",
        "        self.out = out\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout * (1.0 - self.out) * self.out\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Affine:\n",
        "    def __init__(self, W, b):\n",
        "        self.W =W\n",
        "        self.b = b\n",
        "\n",
        "        self.x = None\n",
        "        self.original_x_shape = None\n",
        "        # 权重和偏置参数的导数\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 对应张量\n",
        "        self.original_x_shape = x.shape\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        self.x = x\n",
        "\n",
        "        out = np.dot(self.x, self.W) + self.b\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = np.dot(dout, self.W.T)\n",
        "        self.dW = np.dot(self.x.T, dout)\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "\n",
        "        dx = dx.reshape(*self.original_x_shape)  # 还原输入数据的形状（对应张量）\n",
        "        return dx\n",
        "\n",
        "\n",
        "class SoftmaxWithLoss:\n",
        "    def __init__(self):\n",
        "        self.loss = None\n",
        "        self.y = None # softmax的输出\n",
        "        self.t = None # 监督数据\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        self.t = t\n",
        "        self.y = softmax(x)\n",
        "        self.loss = cross_entropy_error(self.y, self.t)\n",
        "\n",
        "        return self.loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        batch_size = self.t.shape[0]\n",
        "        if self.t.size == self.y.size: # 监督数据是one-hot-vector的情况\n",
        "            dx = (self.y - self.t) / batch_size\n",
        "        else:\n",
        "            dx = self.y.copy()\n",
        "            dx[np.arange(batch_size), self.t] -= 1\n",
        "            dx = dx / batch_size\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Dropout:\n",
        "    \"\"\"\n",
        "    http://arxiv.org/abs/1207.0580\n",
        "    \"\"\"\n",
        "    def __init__(self, dropout_ratio=0.5):\n",
        "        self.dropout_ratio = dropout_ratio\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x, train_flg=True):\n",
        "        if train_flg:\n",
        "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
        "            return x * self.mask\n",
        "        else:\n",
        "            return x * (1.0 - self.dropout_ratio)\n",
        "\n",
        "    def backward(self, dout):\n",
        "        return dout * self.mask\n",
        "\n",
        "\n",
        "class BatchNormalization:\n",
        "    \"\"\"\n",
        "    http://arxiv.org/abs/1502.03167\n",
        "    \"\"\"\n",
        "    def __init__(self, gamma, beta, momentum=0.9, running_mean=None, running_var=None):\n",
        "        self.gamma = gamma\n",
        "        self.beta = beta\n",
        "        self.momentum = momentum\n",
        "        self.input_shape = None # Conv层的情况下为4维，全连接层的情况下为2维\n",
        "\n",
        "        # 测试时使用的平均值和方差\n",
        "        self.running_mean = running_mean\n",
        "        self.running_var = running_var\n",
        "\n",
        "        # backward时使用的中间数据\n",
        "        self.batch_size = None\n",
        "        self.xc = None\n",
        "        self.std = None\n",
        "        self.dgamma = None\n",
        "        self.dbeta = None\n",
        "\n",
        "    def forward(self, x, train_flg=True):\n",
        "        self.input_shape = x.shape\n",
        "        if x.ndim != 2:\n",
        "            N, C, H, W = x.shape\n",
        "            x = x.reshape(N, -1)\n",
        "\n",
        "        out = self.__forward(x, train_flg)\n",
        "\n",
        "        return out.reshape(*self.input_shape)\n",
        "\n",
        "    def __forward(self, x, train_flg):\n",
        "        if self.running_mean is None:\n",
        "            N, D = x.shape\n",
        "            self.running_mean = np.zeros(D)\n",
        "            self.running_var = np.zeros(D)\n",
        "\n",
        "        if train_flg:\n",
        "            mu = x.mean(axis=0)\n",
        "            xc = x - mu\n",
        "            var = np.mean(xc**2, axis=0)\n",
        "            std = np.sqrt(var + 10e-7)\n",
        "            xn = xc / std\n",
        "\n",
        "            self.batch_size = x.shape[0]\n",
        "            self.xc = xc\n",
        "            self.xn = xn\n",
        "            self.std = std\n",
        "            self.running_mean = self.momentum * self.running_mean + (1-self.momentum) * mu\n",
        "            self.running_var = self.momentum * self.running_var + (1-self.momentum) * var\n",
        "        else:\n",
        "            xc = x - self.running_mean\n",
        "            xn = xc / ((np.sqrt(self.running_var + 10e-7)))\n",
        "\n",
        "        out = self.gamma * xn + self.beta\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        if dout.ndim != 2:\n",
        "            N, C, H, W = dout.shape\n",
        "            dout = dout.reshape(N, -1)\n",
        "\n",
        "        dx = self.__backward(dout)\n",
        "\n",
        "        dx = dx.reshape(*self.input_shape)\n",
        "        return dx\n",
        "\n",
        "    def __backward(self, dout):\n",
        "        dbeta = dout.sum(axis=0)\n",
        "        dgamma = np.sum(self.xn * dout, axis=0)\n",
        "        dxn = self.gamma * dout\n",
        "        dxc = dxn / self.std\n",
        "        dstd = -np.sum((dxn * self.xc) / (self.std * self.std), axis=0)\n",
        "        dvar = 0.5 * dstd / self.std\n",
        "        dxc += (2.0 / self.batch_size) * self.xc * dvar\n",
        "        dmu = np.sum(dxc, axis=0)\n",
        "        dx = dxc - dmu / self.batch_size\n",
        "\n",
        "        self.dgamma = dgamma\n",
        "        self.dbeta = dbeta\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Convolution:\n",
        "  def __init__(self, W, b, stride=1, pad=0):\n",
        "    self.W = W\n",
        "    self.b = b\n",
        "    self.stride = stride\n",
        "    self.pad = pad\n",
        "\n",
        "    # 中间数据（backward时使用）\n",
        "    self.x = None\n",
        "    self.col = None\n",
        "    self.col_W = None\n",
        "\n",
        "    # 权重和偏置参数的梯度\n",
        "    self.dW = None\n",
        "    self.db = None\n",
        "\n",
        "  def forward(self, x):\n",
        "    FN, C, FH, FW = self.W.shape\n",
        "    N, C, H, W = x.shape\n",
        "    out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n",
        "    out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n",
        "\n",
        "    col = im2col(x, FH, FW, self.stride, self.pad)\n",
        "    col_W = self.W.reshape(FN, -1).T\n",
        "\n",
        "    out = np.dot(col, col_W) + self.b\n",
        "    out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
        "\n",
        "    self.x = x\n",
        "    self.col = col\n",
        "    self.col_W = col_W\n",
        "\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    FN, C, FH, FW = self.W.shape\n",
        "    dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
        "\n",
        "    self.db = np.sum(dout, axis=0)\n",
        "    self.dW = np.dot(self.col.T, dout)\n",
        "    self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
        "\n",
        "    dcol = np.dot(dout, self.col_W.T)\n",
        "    dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
        "\n",
        "    return dx\n",
        "\n",
        "\n",
        "class Pooling:\n",
        "  def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
        "    self.pool_h = pool_h\n",
        "    self.pool_w = pool_w\n",
        "    self.stride = stride\n",
        "    self.pad = pad\n",
        "\n",
        "    self.x = None\n",
        "    self.arg_max = None\n",
        "\n",
        "  def forward(self, x):\n",
        "    N, C, H, W = x.shape\n",
        "    out_h = int(1 + (H - self.pool_h) / self.stride)\n",
        "    out_w = int(1 + (W - self.pool_w) / self.stride)\n",
        "\n",
        "    col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "    col = col.reshape(-1, self.pool_h*self.pool_w)\n",
        "\n",
        "    arg_max = np.argmax(col, axis=1)\n",
        "    out = np.max(col, axis=1)\n",
        "    out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
        "\n",
        "    self.x = x\n",
        "    self.arg_max = arg_max\n",
        "\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    dout = dout.transpose(0, 2, 3, 1)\n",
        "\n",
        "    pool_size = self.pool_h * self.pool_w\n",
        "    dmax = np.zeros((dout.size, pool_size))\n",
        "    dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
        "    dmax = dmax.reshape(dout.shape + (pool_size,))\n",
        "\n",
        "    dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
        "    dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "\n",
        "    return dx\n"
      ],
      "metadata": {
        "id": "Df3IR3phJtpM"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "梯度函数"
      ],
      "metadata": {
        "id": "tcfp5F9eJ-b-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import numpy as np\n",
        "\n",
        "def _numerical_gradient_1d(f, x):\n",
        "  h = 1e-4 # 0.0001\n",
        "  grad = np.zeros_like(x)\n",
        "\n",
        "  for idx in range(x.size):\n",
        "    tmp_val = x[idx]\n",
        "    x[idx] = float(tmp_val) + h\n",
        "    fxh1 = f(x) # f(x+h)\n",
        "\n",
        "    x[idx] = tmp_val - h\n",
        "    fxh2 = f(x) # f(x-h)\n",
        "    grad[idx] = (fxh1 - fxh2) / (2*h)\n",
        "\n",
        "    x[idx] = tmp_val # 还原值\n",
        "\n",
        "  return grad\n",
        "\n",
        "\n",
        "def numerical_gradient_2d(f, X):\n",
        "  if X.ndim == 1:\n",
        "    return _numerical_gradient_1d(f, X)\n",
        "  else:\n",
        "    grad = np.zeros_like(X)\n",
        "\n",
        "    for idx, x in enumerate(X):\n",
        "        grad[idx] = _numerical_gradient_1d(f, x)\n",
        "\n",
        "    return grad\n",
        "\n",
        "\n",
        "def numerical_gradient(f, x):\n",
        "  h = 1e-4 # 0.0001\n",
        "  grad = np.zeros_like(x)\n",
        "\n",
        "  it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
        "  while not it.finished:\n",
        "    idx = it.multi_index\n",
        "    tmp_val = x[idx]\n",
        "    x[idx] = float(tmp_val) + h\n",
        "    fxh1 = f(x) # f(x+h)\n",
        "\n",
        "    x[idx] = tmp_val - h\n",
        "    fxh2 = f(x) # f(x-h)\n",
        "    grad[idx] = (fxh1 - fxh2) / (2*h)\n",
        "\n",
        "    x[idx] = tmp_val # 还原值\n",
        "    it.iternext()\n",
        "\n",
        "  return grad"
      ],
      "metadata": {
        "id": "o8kCz4uBKAFC"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TwoLayerNet 类"
      ],
      "metadata": {
        "id": "rMPDs1T2yQP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, os\n",
        "sys.path.append(os.pardir)\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "\n",
        "class TwoLayerNet:\n",
        "  def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
        "    # 初始化权重\n",
        "    self.params = {}\n",
        "    self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
        "    self.params['b1'] = np.zeros(hidden_size)\n",
        "    self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
        "    self.params['b2'] = np.zeros(output_size)\n",
        "\n",
        "    # 生成层\n",
        "    self.layers = OrderedDict()\n",
        "    self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
        "    self.layers['Relu1'] = Relu()\n",
        "    self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
        "\n",
        "    self.lastLayer = SoftmaxWithLoss()\n",
        "\n",
        "  def predict(self, x):\n",
        "    for layer in self.layers.values():\n",
        "      x = layer.forward(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "  # x:输入数据, t:监督数据\n",
        "  def loss(self, x, t):\n",
        "    y = self.predict(x)\n",
        "    return self.lastLayer.forward(y, t)\n",
        "\n",
        "  # 精准度\n",
        "  def accuracy(self, x, t):\n",
        "    y = self.predict(x)\n",
        "    y = np.argmax(y, axis=1)\n",
        "    if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
        "    accuracy = np.sum(y == t) / float(x.shape[0])\n",
        "    return accuracy\n",
        "  # x:输入数据, t:监督数据\n",
        "  def numerical_gradient(self, x, t):\n",
        "    loss_W = lambda W: self.loss(x, t)\n",
        "    grads = {}\n",
        "    grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
        "    grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
        "    grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
        "    grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
        "    return grads\n",
        "\n",
        "  # 通过反向传播计算梯度\n",
        "  def gradient(self, x, t):\n",
        "    # forward\n",
        "    self.loss(x, t)\n",
        "    # backward\n",
        "    dout = 1\n",
        "    dout = self.lastLayer.backward(dout)\n",
        "    layers = list(self.layers.values())\n",
        "    layers.reverse()\n",
        "    for layer in layers:\n",
        "      dout = layer.backward(dout)\n",
        "    # 设定\n",
        "    grads = {}\n",
        "    grads['W1'] = self.layers['Affine1'].dW\n",
        "    grads['b1'] = self.layers['Affine1'].db\n",
        "    grads['W2'] = self.layers['Affine2'].dW\n",
        "    grads['b2'] = self.layers['Affine2'].db\n",
        "    return grads\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "A1H4ySzdyX5-"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "训练"
      ],
      "metadata": {
        "id": "7oS5W4heA-Qp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import sys, os\n",
        "sys.path.append(os.pardir)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# 读入数据\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "iters_num = 10000\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100\n",
        "learning_rate = 0.13\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "\n",
        "for i in range(iters_num):\n",
        "  batch_mask = np.random.choice(train_size, batch_size)\n",
        "  x_batch = x_train[batch_mask]\n",
        "  t_batch = t_train[batch_mask]\n",
        "\n",
        "  # 梯度\n",
        "  #grad = network.numerical_gradient(x_batch, t_batch)\n",
        "  grad = network.gradient(x_batch, t_batch)\n",
        "\n",
        "  # 更新\n",
        "  for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "      network.params[key] -= learning_rate * grad[key]\n",
        "\n",
        "  loss = network.loss(x_batch, t_batch)\n",
        "  train_loss_list.append(loss)\n",
        "\n",
        "  if i % iter_per_epoch == 0:\n",
        "    train_acc = network.accuracy(x_train, t_train)\n",
        "    test_acc = network.accuracy(x_test, t_test)\n",
        "    train_acc_list.append(train_acc)\n",
        "    test_acc_list.append(test_acc)\n",
        "    print(train_acc, test_acc)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdzNF7TXA_5M",
        "outputId": "b366f173-e89a-4419-bae1-8dabbe7076d6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.10038333333333334 0.1012\n",
            "0.9070166666666667 0.9102\n",
            "0.9275833333333333 0.9264\n",
            "0.9448166666666666 0.9417\n",
            "0.9524833333333333 0.9489\n",
            "0.9553166666666667 0.951\n",
            "0.96395 0.9561\n",
            "0.9648666666666667 0.9588\n",
            "0.9703333333333334 0.961\n",
            "0.9731666666666666 0.965\n",
            "0.97305 0.9642\n",
            "0.9767833333333333 0.9674\n",
            "0.9768833333333333 0.9679\n",
            "0.9796333333333334 0.9698\n",
            "0.98045 0.9688\n",
            "0.9818833333333333 0.9715\n",
            "0.9815666666666667 0.9699\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "  print(network.params[key].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txUw9OT5Ocna",
        "outputId": "ee7d486c-9f4b-4b9b-e5ed-27c3eb3a98ce"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(784, 50)\n",
            "(50,)\n",
            "(50, 10)\n",
            "(10,)\n"
          ]
        }
      ]
    }
  ]
}